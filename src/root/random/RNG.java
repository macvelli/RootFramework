/*
 * Copyright 2006-2016 Edward Smith
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package root.random;

/**
 * Use a good RNG and build it into your code. From software alone, it is impossible to generate truly random numbers, unless there is a fault in your
 * computer. As the numbers are generated by an algorithm, they are by definition NON-random. What these algorithms generate are PSEUDO-random
 * numbers. The practical definition of pseudo randomness is that the numbers should not be distinguishable from a source of true random numbers in a
 * given application. So one generator may be good enough for one application, but fail badly in another application. True random numbers should not
 * fail in any applications.
 * <p>
 * A nice consequence of combining different RNGs is that a statistical flaw in any one of the component generators is likely to be covered up by the
 * other generators. Combining different RNGs is now considered sound practice in designing good RNGs by many experts in the field. I would propose
 * the KISS RNG as representing the minimum acceptable standard in random number generation.
 * <p>
 * Don't forget, that for cryptographic work, most standard RNGs (including MT) are worthless - but if you are working in cryptography or security
 * then hopefully you already know everything about secure random number generation!
 * <p>
 * If efficiency is of concern, one must criticize the Random class as being slower than necessary by the generalization of wrapper methods that hide
 * the relatively simple multiply-add-modulus operation of a linear congruential generator inside several layers of method calls.
 * <p>
 * A family of methods that returns vectors of pseudorandom numbers could produce even faster generation, since it could completely eliminate the
 * method call overhead.
 * <p>
 * Properly seed your generator. Even the state-of-the-art Mersenne Twister ran into problems early on because the authors had neglected the issue of
 * proper seeding. Even now, correctly seeding MT is a non-trivial problem (MT effectively has 624 integer seed variables which need to be properly
 * initialized at the start!). This rule is VITAL if you are going to run parallel simulations on a Beowulf cluster for example.
 * <p>
 * The simplest way to seed a RNG is to take something like the current time e.g. using the time() function found in Unix and most C libraries, which
 * returns a 32-bit integer giving the number of seconds since 1st Jan 1970. Early versions of Perl used this idea for example. What's wrong with
 * this? Well, it's more or less acceptable if you are running a program once a day, but think of what happens when you submit 1000 jobs to a Beowulf
 * cluster in one batch. Hundreds of jobs on different nodes will be starting at almost exactly the same time, therefore many of your jobs will be
 * starting with exactly the same seed and therefore those that have the same seed will generate exactly the same results (assuming your code has no
 * bugs in it). Not only is it a waste of CPU time to repeat the exact same calculation many times, but if you don't know it has happened, any
 * statistics you carry out on the final data will be biased by the repetitions. Think of molecular dynamics as a good example, hundreds of
 * simulations could be running, all generating exactly the same trajectory because the RNGs were unknowingly started in exactly the same state.
 * <p>
 * Note that even if you manage to come up with a way of generating truly random 32-bit seed values you are still not necessarily safe. Remember the
 * old Birthday Paradox i.e. how many people do you need in a room for it to be more likely than not that at least two of them have the same birthday?
 * Remember the answer is surprisingly small (n=23). The same thing applies to random number seeds. If you generate just 216 (=65536) random 32-bit
 * seeds you would expect there to be at least one repetition. So if you submit many thousands of jobs to a cluster even with completely random 32-bit
 * seeds you run a risk that two or more of the simulations will start with identical seeds.
 * <p>
 * To avoid this, you firstly need more state bits in your RNG i.e. your RNG has to use seeds summing to at least 64-bits of information to avoid
 * repeating the same seed (and to reduce the chance of the sequences of random numbers overlapping in two simulations). In the case of the KISS
 * generator above, if you randomize the 3 main seed variables (x,y,z) you will get 96 bits of seed in total. But how do you set these values
 * initially?
 * <p>
 * You could use a combination of time of day and the microsecond system clock (both from gettimeofday()) plus say process ID (getpid()) and host ID
 * (gethostid() - to avoid generating the same seeds on two different compute nodes) or any other varying system values you can think of. Some of
 * these values won't change much from run to run and so the Birthday Paradox may still catch you out, but the above values are probably good enough
 * for many applications. Don't fall into the trap of just combining these variables into a single 32-bit seed (e.g. by adding them together) - you
 * need to use them separately to form 64 or more bits of seed information.
 * <p>
 * Another simple trick is to pipe some system data that frequently changes into md5sum e.g.:
 * <p>
 * <code>ps -ef | md5sum</code>
 * <p>
 * In many operating systems today, however, there is an ideal solution available: /dev/random
 * <p>
 * If you read 4 bytes from this device then you will get a nicely random and unpredictable 32-bit seed value, and you can read as many bytes as you
 * need. This device uses system hardware timings (e.g. hard disk access times and intervals between key presses) to produce entropy and is as close
 * as you will get to a source of true random numbers without a radioactive source and a Geiger counter!
 * <p>
 * Note that /dev/random will block until it thinks it has accumulated enough entropy. This could delay your program by several seconds or even
 * minutes, so you will probably be better off using /dev/urandom instead. This always returns random data immediately, but with a risk that the
 * numbers might be more "predictable" than from /dev/random. Unless you are into cryptography this shouldn't worry you, and the results from
 * /dev/urandom are still going to be much better than from any of the simpler methods described above. The /dev/urandom device could even be used as
 * the sole source of random numbers in your program, but is probably best used just to set the seeds for something like the KISS generator as reading
 * from these devices can be slow on some systems.
 * <p>
 * For Windows systems, it is possible to use the Cryptographic Application Programming Interface to generate secure random numbers in place of
 * /dev/random. The simplest option if using Microsoft C++ is to use the rand_s() library function (no relation to the infamous rand() function -
 * careful you don't mistype!) to generate good quality secure random numbers for seeding purposes.
 * <p>
 * As a footnote to this, it's worth pointing out that another simple solution to the seeding problem is to allow the seed values to be set externally
 * e.g. through a command-line parameter or configuration file. It is then the responsibility of the user to ensure that a unique seed is used every
 * time the program is run. A danger here is that the user could specify small low complexity seed values e.g. 1, 2, 3, 4 etc. To counter this, the
 * input seed values should be randomized e.g. using a secure hash such as MD5, or if the seeds are used directly, a long "warm up" of the generator
 * should be carried out first (see later).
 *
 * @author Edward Smith
 * @version 0.5
 * @since 0.5
 */
public interface RNG {

	/**
	 * Notably absent from this list is a getSeed() method: well-designed packages for pseudorandom number generation should always have such a
	 * function or method. Without it, it is impossible to save the state of a generator so that it can be started reproducibly.
	 *
	 * @return the byte[] used to seed the pseudorandom number generator
	 */
	byte[] getSeed();

	/**
	 * Returns the next <code>boolean</code> from the pseudorandom number generator.
	 *
	 * @return the next <code>boolean</code> from the pseudorandom number generator
	 */
	boolean nextBoolean();

	/**
	 * Populates the <code>byte[]</code> with values generated by the pseudorandom number generator.
	 *
	 * @param byteArray
	 *            the <code>byte[]</code> to populate
	 * @param offset
	 *            the offset to begin population from
	 * @param length
	 *            the number of bytes to populate
	 */
	void nextBytes(final byte[] byteArray, final int offset, final int length);

	/**
	 * L'Ecuyer says:
	 *
	 * We consider here the following widely-used generators.
	 *
	 * Java. This is the generator used to implement the method nextDouble in the class java.util.Random of the Java standard library (see
	 * https://docs.oracle.com/javase/7/docs/api/java/util/Random.html) It is based on a linear recurrence with period length 2^48, but each output
	 * value is constructed by taking two successive values from the linear recurrence, as follows:
	 *
	 * x_{i+1} = (25214903917 x_i + 11) mod 2^48 u_i = (2^27 floor(x_{2i}/2^22) + floor(x_{2i}+1/2^21) )/2^53
	 *
	 * Note that the generator rand48 in the Unix standard library uses exactly the same recurrence, but produces its output simply via
	 *
	 * u_i = x_i/2^48
	 *
	 * @return
	 */
	double nextDouble();

	/**
	 * Generating random floating point numbers
	 *
	 * The following C code generate a random (double precision) floating point number 0 <= x < 1:
	 *
	 * double x; x = JKISS() / 4294967296.0;
	 *
	 * Works fine, and in fact will often produce reasonable results even with somewhat defective random number generators. However, 32-bits are not
	 * enough to generate all possible double precision float values between 0 and 1 (more than enough for single precision floats though). This may
	 * not be a problem in your application, but if it is, you will need to call the RNG twice and combine the results to generate a full 53-bit
	 * precision double e.g. as follows:
	 *
	 * double uni_dblflt() { double x; unsigned int a, b; a = JKISS() >> 6; // Upper 26 bits b = JKISS() >> 5; // Upper 27 bits x = (a * 134217728.0 +
	 * b) / 9007199254740992.0;
	 *
	 * return x; }
	 *
	 * @return
	 */
	float nextFloat();

	/**
	 * If speed really matters, then it is possible to avoid the floating point scaling by directly manipulating the bits that make up the significand
	 * (mantissa) of the floating point variable. Unless every cycle counts, this is just not worth the hassle (or loss of portability) in my book.
	 * However, here are examples for generating single or double precision floats quickly:
	 *
	 * // Quickly generate random single precision float 0<=x<1 by type punning float uni_qsingflt() { float x; unsigned int a; a = JKISS() >> 9; //
	 * Take upper 23 bits *((unsigned int *)&x) = a | 0x3F800000; // Make a float from bits return x-1.0F; }
	 *
	 * // Quickly generate random double precision float 0<=x<1 by type punning double uni_qdblflt() { double x; unsigned long long a; a = ((unsigned
	 * long long)JKISS()<<32) + JKISS(); a = (a >> 12) | 0x3FF0000000000000ULL; // Take upper 52 bits *((unsigned long long *)&x) = a; // Make a
	 * double from bits return x-1.0; }
	 *
	 * @param floats
	 * @return
	 */

	/**
	 * Generate random integers in the range {@code 0 <= int < size}.
	 * <p>
	 * With a good RNG, you can generate random indexes like this:
	 * <p>
	 * {@code int rand = JKISS() % size;}
	 * <p>
	 * This will generate uniformly distributed random integers between 0 and size inclusive. Actually, the integers will not be perfectly uniformly
	 * distributed unless the divisor is a factor of 2^32, but the bias is negligible if the divisor is much smaller than 2^32.
	 * <p>
	 * If you are stuck with a system generator or some other defective generator, NEVER use this method. Use something like this:
	 * <p>
	 * {@code int rand = (int)(10.0 * random() / (RAND_MAX + 1));}
	 * <p>
	 * In other words, convert the 32-bit random integer to a floating point number 0 <= x < 1 and use this to generate your final integer. This way
	 * all of the bits are used. If you used the first approach with the old C-library rand() you would generate odd numbers followed by even numbers
	 * every time! It's still better to just use a good RNG in the first place (and avoid the floating point arithmetic).
	 *
	 * @param size
	 *            the upper bound (exclusive) of the index
	 * @return a random integer in the range {@code 0 <= int < size}
	 */
	int nextIndex(int size);

	/**
	 * Returns the next {@code int} from the pseudorandom number generator.
	 *
	 * @return the next {@code int} from the pseudorandom number generator
	 */
	int nextInt();

	/**
	 * Returns the next {@code long} from the pseudorandom number generator.
	 *
	 * @return the next {@code long} from the pseudorandom number generator
	 */
	long nextLong();

	/**
	 * Generate random integers in the range {@code from <= int < to}.
	 *
	 * @param from
	 *            the lower bound (inclusive) of the range
	 * @param to
	 *            the upper bound (exclusive) of the range
	 * @return a random integer in the range {@code from <= int < to}
	 */
	int nextRange(int from, int to);

	/**
	 * Generating Gaussian Deviates
	 *
	 * In some applications, random numbers need to be generated according to the normal distribution rather than uniformly over the range 0.0-1.0.
	 * The most commonly used method for this is the Box-Mueller transform, which returns numbers with a mean of 0.0 and standard deviation of 1.0:
	 *
	 * #include <math.h> // Generate gaussian deviate with mean 0 and stdev 1 double gaussrnd() { double x, y, r; do { x = 2.0 * uni_dblflt() - 1.0; y
	 * = 2.0 * uni_dblflt() - 1.0; r = x * x + y * y; } while (r == 0.0 || r >= 1.0); r = sqrt((-2.0 * log(r)) / r); return x * r; }
	 *
	 * It should be noted that a much faster (though more elaborate) way of generating Gaussian deviates (the Ziggurat method) has been proposed by
	 * Marsaglia & Tsang (see http://www.jstatsoft.org/v05/i08). A Fortran90 implementation can be found here:
	 *
	 * http://www.netlib.org/random/ziggurat.f90.
	 */

	/**
	 * Unbiased Shuffling
	 *
	 * Probably the most common need for random numbers in bioinformatics applications is in shuffling e.g. shuffling the amino acids in a protein
	 * sequence. There is a correct way to shuffle, called the Fisher-Yates or Knuth shuffle, which ensures that all permutations are sampled
	 * uniformly. Here is an example of how to shuffle a string/sequence correctly:
	 *
	 * //Perform random shuffle on sequence (string) s void shufseq(char *s, int length) { int i, r; char temp; for (i = length - 1; i >= 1; i--) { //
	 * 0 <= r <= i r = (double) JKISS() * (i + 1) / 4294967296.0; temp = s[i]; s[i] = s[r]; s[r] = temp; } }
	 */

	/**
	 * Don't use "too many" random numbers
	 *
	 * Although it's clear that it's a bad idea to use a longer sequence of random numbers than the period of a given RNG (because the sequence
	 * obviously starts repeating), there are arguments in the literature that suggest that it's a bad idea to use more than even a small fraction of
	 * the RNG's period in a single simulation. At least a part of this argument is that you ideally want to avoid using overlapping sequences of
	 * random numbers from one run to the next, though there is a bit more to it than that. For a RNG with period p, Knuth suggests the limit should
	 * be p/1000, which is not a problem for decent RNGs with periods of say 2^50 or more. However, Ripley has argued that a safer limit should be
	 * 1/200 sqrt(p), and there are some suggestions that in extreme cases an even safer limit might be cubert(p) . Today, a single application
	 * running for a month on a very fast computer might conceivably be able to use 10^12 random numbers (that's assuming it does little more than
	 * generate random numbers!). This suggests that as a worst-case-scenario, a minimum period of at least 2^120 might be needed for simulations of
	 * that length. All of the RNGs mentioned in these notes have sufficient periods. However, for much longer simulations, even though you would
	 * probably be wise to use MT or some other super-long-period generator (e.g. see Appendix 1), it's simple to construct a KISS generator with much
	 * longer period e.g. 2^191 with little loss of speed on modern 64-bit CPUs, by upgrading the xor-shift register and linear congruential
	 * generators to 64-bit operations:
	 *
	 * //Public domain code for JLKISS RNG - long period KISS RNG with 64-bit operations unsigned long long x = 123456789123ULL,y = 987654321987ULL;
	 * // Seed variables unsigned int z = 43219876,c = 6543217; // Seed variables unsigned int JLKISS() { unsigned long long t; x =
	 * 1490024343005336237ULL * x + 123456789; y ^= y << 21; y ^= y >> 17; y ^= y << 30; // Do not set y=0! t = 4294584393ULL * z + c; c = t >> 32; z
	 * = t; // Avoid z=c=0! return (unsigned int)(x>>32) + (unsigned int)y + z; // Return 32-bit result }
	 *
	 * Although the above code makes use of 64-bit variables, it still produces 32-bit integers. If, for some reason, you want to produce 64-bit
	 * random integers directly, then an extra multiply-with-carry generator can be used to produce a full 64-bit result (period this time is 2^250):
	 *
	 * // Public domain code for JLKISS64 RNG - long period KISS RNG producing 64-bit results unsigned long long x = 123456789123ULL,y =
	 * 987654321987ULL; // Seed variables unsigned int z1 = 43219876, c1 = 6543217, z2 = 21987643, c2 = 1732654; // Seed variables unsigned long long
	 * JLKISS64() { unsigned long long t; x = 1490024343005336237ULL * x + 123456789; y ^= y << 21; y ^= y >> 17; y ^= y << 30; // Do not set y=0! t =
	 * 4294584393ULL * z1 + c1; c1 = t >> 32; z1 = t; t = 4246477509ULL * z2 + c2; c2 = t >> 32; z2 = t; return x + y + z1 + ((unsigned long long)z2
	 * << 32); // Return 64-bit result }
	 */

	/**
	 * Appendix 1
	 *
	 * Below are some other short, interesting and good RNGs that have been posted to Usenet (again by the prolific George Marsaglia). I thought I'd
	 * append them here, as despite being short and simple RNGs, they have huge periods (and pass all of the standard tests). They are based around
	 * the multiply-with-carry algorithm, rather than large shift registers, so they are useful alternative methods to compare against MT when you do
	 * have a need for a very long period generator. The only difficulty in using these generators (in common with other super-long-period generators)
	 * is in proper seeding i.e. how to initialize the large state arrays (Q), which should be pre-filled with random seed values. They can be filled
	 * using another RNG such as the KISS generator or using /dev/(u)random. If there are any concerns about the randomness of the starting state,
	 * then a warm-up of the generator should be carried out (at least 4 * 256 values should be discarded at the start for MWC256, at least 4 * 4096
	 * discarded for CMWC4096 and 4 * 41790 for SuperKISS).
	 *
	 * Firstly, the baby MWC256 achieves a period of 2^8222 by using a state array Q of 256 32-bit unsigned integers.
	 *
	 * // MWC256 from Usenet posting by G. Marsaglia - Period 2^8222 static unsigned int Q[256], c=362436; unsigned int MWC256(void) { unsigned long
	 * long t; static unsigned char i=255; t = 809430660ULL * Q[++i] + c; c = (t>>32); return (Q[i]=t); }
	 *
	 * CMWC4096 achieves a period of 2^131086 (bigger even than that of MT) by using a state array Q of 4096 32-bit unsigned integers.
	 *
	 * // CMWC4096 from Usenet posting by G. Marsaglia - Period 2^131086 static unsigned int Q[4096],c=362436; unsigned int CMWC4096(void) { unsigned
	 * long long t; unsigned int x; static unsigned int i=4095; i=(i+1)&4095; t=18782ULL*Q[i]+c; c=(t>>32); x=t+c; if (x<c) { x++; c++; } return (Q[i]
	 * = 0xFFFFFFFEU-x); }
	 *
	 * Super KISS achieves a period of 54767 x 2^1337279 (vastly bigger than that of MT) by using a state array Q of 41790 32-bit unsigned integers.
	 *
	 * // Super KISS based on Usenet posting by G. Marsaglia - Period 54767 * 2^1337279 static unsigned int
	 * Q[41790],indx=41790,carry=362436,xcng=1236789,xs=521288629; // Fill Q array with random unsigned 32-bit ints and return first element unsigned
	 * int refill() { int i; unsigned long long t; for (i=0;i<41790;i++) { t = 7010176ULL * Q[i] + carry; carry = (t>>32); Q[i] = ~t; } indx=1; return
	 * (Q[0]); }
	 *
	 * // Return 32-bit random integer, calls refill() when needed unsigned int SuperKISS() { xcng = 69069 * xcng + 123; xs ^= xs<<13; xs ^= xs>>17;
	 * xs ^= xs>>5; return (indx<41790 ? Q[indx++] : refill()) + xcng + xs; }
	 */

} // End RNG
